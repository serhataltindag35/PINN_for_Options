{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***CONNECTING TO GOOGLE DRIVE***"
      ],
      "metadata": {
        "id": "0lim52uYSO4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "bPx2cu_Y-Gk4",
        "outputId": "321484bc-bb27-467a-a068-9b84ef252cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         )\n\u001b[0;32m--> 272\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***LIBRARIES***"
      ],
      "metadata": {
        "id": "Mqx0Zp5RTDsC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcDiBAjHHdNp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as Func\n",
        "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "#torch.manual_seed(123)\n",
        "np.random.seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMR_Z90-PudB"
      },
      "source": [
        "# ***MODEL***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BESwU-WRHoAS"
      },
      "outputs": [],
      "source": [
        "# Define the ABU activation function for the SABR PDE\n",
        "class SinSoftplusActivation(nn.Module):\n",
        "    def forward(self, input):\n",
        "        new_act_f = 0.005*torch.sin(input) + 1*torch.nn.functional.softplus(input) + 0*torch.nn.functional.tanh(input) + 0.00*torch.nn.functional.gelu(input)\n",
        "        return new_act_f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9zToy_-Hpip"
      },
      "outputs": [],
      "source": [
        "class Alternative_FCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Alternative_FCN, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm1d(3)\n",
        "        self.fc1 = nn.Linear(3, 200)\n",
        "        self.act1 = SinSoftplusActivation()\n",
        "        self.bn2 = nn.BatchNorm1d(200)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.act2 = SinSoftplusActivation()\n",
        "        self.bn3 = nn.BatchNorm1d(200)\n",
        "        self.fc3 = nn.Linear(200, 200)\n",
        "        self.act3 = SinSoftplusActivation()\n",
        "        self.bn4 = nn.BatchNorm1d(200)\n",
        "        self.fc4 = nn.Linear(200, 200)\n",
        "        self.act4 = SinSoftplusActivation()\n",
        "        self.bn5 = nn.BatchNorm1d(200)\n",
        "        self.fc5 = nn.Linear(200, 200)\n",
        "        self.act5 = SinSoftplusActivation()\n",
        "        self.bn6 = nn.BatchNorm1d(200)\n",
        "        self.fc6 = nn.Linear(200, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.act3(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.act4(x)\n",
        "        x = self.fc5(x)\n",
        "        x = self.act5(x)\n",
        "        x = self.fc6(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT_ANQAePmCK"
      },
      "source": [
        "# ***RANDOM DATASET GENERATION FOR PDE AND OTHER CONDITIONS***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XUewANPHllD"
      },
      "outputs": [],
      "source": [
        "def generate_points_per_condition(N=1000, S_min=0.0, S_max=150.0,\n",
        "                                sigma_min=0.0, sigma_max=0.5, t=1.0):\n",
        "    \"\"\"\n",
        "    Generates 1,000 random points for each:\n",
        "    - Interior domain\n",
        "    - S=0 boundary\n",
        "    - S=S_max boundary\n",
        "    - σ=0 boundary\n",
        "    - σ=σ_max boundary\n",
        "    - Initial condition (t=T)\n",
        "    \"\"\"\n",
        "    # --- Interior points (S, σ, t all random) ---\n",
        "    S_int = torch.rand(N, 1) * (S_max - S_min) + S_min\n",
        "    sigma_int = torch.rand(N, 1) * (sigma_max - sigma_min) + sigma_min\n",
        "    sigma_int = torch.pow(sigma_int, 2)\n",
        "    t_int = torch.rand(N, 1) * t\n",
        "    interior = torch.cat([S_int, sigma_int, t_int], dim=1)\n",
        "\n",
        "    # --- Boundary: S=0 (σ and t random) ---\n",
        "    S0 = torch.zeros(N, 1)\n",
        "    sigma_S0 = torch.rand(N, 1) * (sigma_max - sigma_min) + sigma_min\n",
        "    sigma_S0 = torch.pow(sigma_S0, 2)\n",
        "    t_S0 = torch.rand(N, 1) * t\n",
        "    bd_S0 = torch.cat([S0, sigma_S0, t_S0], dim=1)\n",
        "\n",
        "    # --- Boundary: S=S_max (σ and t random) ---\n",
        "    Smax = torch.ones(N, 1) * S_max\n",
        "    sigma_Smax = torch.rand(N, 1) * (sigma_max - sigma_min) + sigma_min\n",
        "    sigma_Smax = torch.pow(sigma_Smax, 2)\n",
        "    t_Smax = torch.rand(N, 1) * t\n",
        "    bd_Smax = torch.cat([Smax, sigma_Smax, t_Smax], dim=1)\n",
        "\n",
        "    # --- Boundary: σ=0 (S and t random) ---\n",
        "    S_sigma0 = torch.rand(N, 1) * (S_max - S_min) + S_min\n",
        "    sigma0 = torch.zeros(N, 1)\n",
        "    t_sigma0 = torch.rand(N, 1) * t\n",
        "    bd_sigma0 = torch.cat([S_sigma0, sigma0, t_sigma0], dim=1)\n",
        "\n",
        "    # --- Boundary: σ=σ_max (S and t random) ---\n",
        "    S_sigmamax = torch.rand(N, 1) * (S_max - S_min) + S_min\n",
        "    sigmamax = torch.ones(N, 1) * sigma_max\n",
        "    sigmamax = torch.pow(sigmamax, 2)\n",
        "    t_sigmamax = torch.rand(N, 1) * t\n",
        "    bd_sigmamax = torch.cat([S_sigmamax, sigmamax, t_sigmamax], dim=1)\n",
        "\n",
        "    # --- Initial condition: t=T (S and σ random) ---\n",
        "    S_initial = torch.rand(N, 1) * (S_max - S_min) + S_min\n",
        "    sigma_initial = torch.rand(N, 1) * (sigma_max - sigma_min) + sigma_min\n",
        "    sigma_initial = torch.pow(sigma_initial, 2)\n",
        "    t_initial = torch.zeros(N, 1)\n",
        "    initial = torch.cat([S_initial, sigma_initial, t_initial], dim=1)\n",
        "\n",
        "    # Combine all (total points = 6*N)\n",
        "    all_points = torch.cat([\n",
        "        interior, bd_S0, bd_Smax, bd_sigma0, bd_sigmamax, initial\n",
        "    ], dim=0).requires_grad_(True)\n",
        "\n",
        "    return {\n",
        "        'interior': interior,\n",
        "        'bd_S0': bd_S0,\n",
        "        'bd_Smax': bd_Smax,\n",
        "        'bd_sigma0': bd_sigma0,\n",
        "        'bd_sigmamax': bd_sigmamax,\n",
        "        'initial': initial,\n",
        "        'all': all_points\n",
        "    }\n",
        "\n",
        "# Generate 1,000 points per condition\n",
        "data = generate_points_per_condition(N=365)\n",
        "\n",
        "# Verify\n",
        "print(f\"Total points: {len(data['all'])}\")\n",
        "print(f\"Interior: {len(data['interior'])} points (S∈[{data['interior'][:,0].min():.1f}, {data['interior'][:,0].max():.1f}])\")\n",
        "print(f\"S=0 boundary: {len(data['bd_S0'])} points (σ∈[{data['bd_S0'][:,1].min():.1f}, {data['bd_S0'][:,1].max():.1f}])\")\n",
        "print(f\"σ=0 boundary: {len(data['bd_sigma0'])} points (S∈[{data['bd_sigma0'][:,0].min():.1f}, {data['bd_sigma0'][:,0].max():.1f}])\")\n",
        "\n",
        "# Plot S-σ distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(data['interior'][:,0], data['interior'][:,1], label='Interior', alpha=0.3)\n",
        "plt.scatter(data['bd_S0'][:,0], data['bd_S0'][:,1], label='S=0 boundary', alpha=0.5)\n",
        "plt.scatter(data['bd_sigma0'][:,0], data['bd_sigma0'][:,1], label='σ=0 boundary', alpha=0.5)\n",
        "plt.xlabel(\"Asset Price (S)\"), plt.ylabel(\"Volatility (σ)\")\n",
        "plt.title(\"Training Points Distribution (1,000 per condition)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92rz4WVqPfxv"
      },
      "source": [
        "# ***PDE AND PINN CONDITIONS***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_derivatives_heston(model, S, v, t):\n",
        "    inputs = torch.cat([S, v, t], dim=1)\n",
        "    # Compute C (option price)\n",
        "    C = model(inputs)\n",
        "\n",
        "    # Compute partial derivatives using autograd\n",
        "    C_S = torch.autograd.grad(C, S, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
        "    C_v = torch.autograd.grad(C, v, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
        "    C_t = torch.autograd.grad(C, t, grad_outputs=torch.ones_like(C), create_graph=True)[0]\n",
        "\n",
        "    C_SS = torch.autograd.grad(C_S, S, grad_outputs=torch.ones_like(C_S), create_graph=True)[0]\n",
        "    C_vv = torch.autograd.grad(C_v, v, grad_outputs=torch.ones_like(C_v), create_graph=True)[0]\n",
        "    C_Sv = torch.autograd.grad(C_S, v, grad_outputs=torch.ones_like(C_S), create_graph=True)[0]\n",
        "\n",
        "    return C, C_S, C_v, C_t, C_SS, C_Sv, C_vv\n",
        "\n",
        "    # Define the Heston PDE\n",
        "def heston_pde(model, S, v, t, kappa, theta, sigma, rho, r):\n",
        "    C, C_S, C_v, C_t, C_SS, C_Sv, C_vv = compute_derivatives_heston(model, S, v, t)\n",
        "    rho = torch.tensor(rho, dtype=torch.float32, device=S.device)\n",
        "    sigma = torch.tensor(sigma, dtype=torch.float32, device=S.device)\n",
        "    kappa = torch.tensor(kappa, dtype=torch.float32, device=S.device)\n",
        "    theta = torch.tensor(theta, dtype=torch.float32, device=S.device)\n",
        "    r = torch.tensor(r, dtype=torch.float32, device=S.device)\n",
        "    # Compute the terms of the Heston PDE\n",
        "    term1 = C_t\n",
        "    term2 = 0.5 * v * S**2 * C_SS\n",
        "    term3 = rho * sigma * v * S * C_Sv\n",
        "    term4 = 0.5 * sigma**2 * v * C_vv\n",
        "    term5 = r * S * C_S\n",
        "    term6 = kappa * (theta - v) * C_v\n",
        "    term7 = -r * C\n",
        "\n",
        "    # Heston PDE that equals zero\n",
        "    residual = term1 + term2 + term3 + term4 + term5 + term6 + term7\n",
        "    return residual\n",
        "\n",
        "# Define boundary conditions\n",
        "def Smin_conditions(model, inputs_S_min):\n",
        "    # Boundary condition for S = S_min\n",
        "    boundary_S_min = model(inputs_S_min)  # V(t, F_min, alpha) = 0\n",
        "    boundary_S_min = torch.mean(boundary_S_min**2)\n",
        "\n",
        "    return boundary_S_min\n",
        "\n",
        "# Define boundary conditions\n",
        "def Smax_conditions2(model,  inputs_S_max):\n",
        "    # Boundary condition for S = S_max\n",
        "    fSmax = inputs_S_max[:, 0].requires_grad_(True).reshape(-1, 1)\n",
        "    V_S_max = model(inputs_S_max)\n",
        "    V_S_max_dx = torch.autograd.grad(V_S_max, inputs_S_max, torch.ones_like(V_S_max), create_graph=True)[0]\n",
        "    dVdS = V_S_max_dx[:, 0].reshape(-1, 1)\n",
        "    boundary_S_max = dVdS - 1\n",
        "    boundary_S_max = torch.mean(boundary_S_max**2)\n",
        "    return boundary_S_max\n",
        "\n",
        "    # Define boundary conditions\n",
        "def Smax_conditions(model,  inputs_S_max):\n",
        "    # Boundary condition for S = S_max\n",
        "    fSmax = inputs_S_max[:, 0].requires_grad_(True).reshape(-1, 1)\n",
        "    V_S_max = model(inputs_S_max)\n",
        "    V_S_max_dx = torch.autograd.grad(V_S_max, inputs_S_max, torch.ones_like(V_S_max), create_graph=True)[0]\n",
        "    V_S_max_d2x = torch.autograd.grad(V_S_max_dx, inputs_S_max, torch.ones_like(V_S_max_dx), create_graph=True)[0]\n",
        "    dSmaxd2x = V_S_max_d2x[:, 0].reshape(-1, 1)\n",
        "    boundary_S_max = torch.mean(dSmaxd2x**2)\n",
        "    return boundary_S_max\n",
        "\n",
        "# Define boundary conditions\n",
        "def v0_conditions(model, inputs_v_min, K, r):\n",
        "    # Boundary condition for alpha = 0\n",
        "    Vmin_S = inputs_v_min[:, 0].reshape(-1, 1)\n",
        "    Vmin_t = inputs_v_min[:, 2].reshape(-1, 1)\n",
        "    V_v_min = model(inputs_v_min)\n",
        "    V_v_min_dt = torch.autograd.grad(V_v_min, inputs_v_min, torch.ones_like(V_v_min), create_graph=True)[0]\n",
        "    dVmindt = V_v_min_dt[:, 2].reshape(-1, 1)\n",
        "    #boundary_alpha_min = V_alpha_min_dt\n",
        "    boundary_v_min = V_v_min - torch.maximum(Vmin_S - K*torch.exp(-r*Vmin_t), torch.tensor(0.0))\n",
        "    boundary_v_min = torch.mean(boundary_v_min**2)\n",
        "\n",
        "    return boundary_v_min\n",
        "# Define boundary conditions\n",
        "def Vmax_conditions(model, inputs_v_max):\n",
        "    # Boundary condition for alpha = 0\n",
        "    Vmax_S = inputs_v_max[:, 0].reshape(-1, 1)\n",
        "    V_v_max = model(inputs_v_max)\n",
        "    boundary_v_max = V_v_max - Vmax_S\n",
        "    boundary_v_max = torch.mean(boundary_v_max**2)\n",
        "\n",
        "    return boundary_v_max\n",
        "\n",
        "def IC_european_call(S, K):\n",
        "    val = torch.maximum(S - K, torch.tensor(0.0))\n",
        "    return val.view(-1, 1).requires_grad_(True)"
      ],
      "metadata": {
        "id": "bWFYiuNC7zDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NAtl_ljPTfe"
      },
      "source": [
        "# ***ANALYTICAL BS - MANTO CARLO - SABR PATH GENERATION***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnZ_B5pDH4aB"
      },
      "outputs": [],
      "source": [
        "# ANALYTICAL BS OPTION PRICE\n",
        "\n",
        "def black_scholes_option_price(S, K, T, r, sigma, option_type='call'):\n",
        "\n",
        "    # Calculate d1 and d2\n",
        "    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T+0.00000001))\n",
        "    d2 = d1 - sigma * np.sqrt(T)\n",
        "\n",
        "    if option_type == 'call':\n",
        "        # Calculate the price of a European call option\n",
        "        option_price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n",
        "    elif option_type == 'put':\n",
        "        # Calculate the price of a European put option\n",
        "        option_price = K * np.exp(-r * T) * norm.cdf(-d2) - S * norm.cdf(-d1)\n",
        "    else:\n",
        "        raise ValueError(\"option_type must be 'call' or 'put'\")\n",
        "\n",
        "    return option_price\n",
        "\n",
        "# SABR PATH SIMULATION\n",
        "\n",
        "def simulate_sabr_path(beta, nu, rho,\n",
        "                       F0, sigma0, strike_price,\n",
        "                       T, N, r, plot=True):\n",
        "    np.random.seed(123)\n",
        "    dt = T / N\n",
        "    dW1 = np.random.normal(0, np.sqrt(dt), N)\n",
        "    dW2 = rho * dW1 + np.sqrt(1 - rho**2) * np.random.normal(0, np.sqrt(dt), N)\n",
        "\n",
        "    if plot:\n",
        "        plt.plot(dW1, label='dW1')\n",
        "        plt.plot(dW2, label='dW2')\n",
        "        plt.title(\"Brownian Motions\")\n",
        "        plt.xlabel(\"Steps (N)\")\n",
        "        plt.ylabel(\"W_1 and W_2\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    F = np.zeros(N)\n",
        "    sigma = np.zeros(N)\n",
        "    F[0] = F0\n",
        "    sigma[0] = sigma0\n",
        "\n",
        "    for t in range(1, N):\n",
        "        #discount_F = F0 * np.exp(r * (t-1))\n",
        "        F[t] = F[t-1] + sigma[t-1] * (F[t-1])**beta * dW1[t-1]\n",
        "        sigma[t] = sigma[t-1] + nu * sigma[t-1] * dW2[t-1]\n",
        "\n",
        "    return F, sigma\n",
        "\n",
        "# MONTE CARLO OPTION PRICE\n",
        "\n",
        "def sabr_european_call_mc(S0_array, sigma0_array, T_array, K, r,\n",
        "                          beta, nu, rho, N, M, confidence=0.95):\n",
        "\n",
        "    S0_array = np.asarray(S0_array).flatten()\n",
        "    sigma0_array = np.asarray(sigma0_array).flatten()\n",
        "    T_array = np.asarray(T_array).flatten()\n",
        "\n",
        "    n_samples = len(S0_array)\n",
        "    prices = np.zeros(n_samples)\n",
        "    conf_intervals = []\n",
        "\n",
        "    z_score = norm.ppf(1 - (1 - confidence) / 2)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        S0 = S0_array[i]\n",
        "        alpha0 = sigma0_array[i]\n",
        "        T = T_array[i]\n",
        "        dt = T / N\n",
        "        sqrt_dt = np.sqrt(dt)\n",
        "\n",
        "        # Initialize arrays\n",
        "        S = np.zeros((M, N+1))\n",
        "        alpha = np.zeros((M, N+1))\n",
        "        S[:, 0] = S0\n",
        "        alpha[:, 0] = alpha0\n",
        "\n",
        "        for t in range(N):\n",
        "            z1 = np.random.randn(M)\n",
        "            z2 = np.random.randn(M)\n",
        "            dW1 = z1\n",
        "            dW2 = rho * z1 + np.sqrt(1 - rho**2) * z2\n",
        "\n",
        "            alpha[:, t+1] = alpha[:, t] + nu * alpha[:, t] * dW2 * sqrt_dt\n",
        "            S[:, t] = np.maximum(S[:, t], 1e-12)\n",
        "            sigma_S = alpha[:, t] * (S[:, t]**(beta))\n",
        "            mu_S = r * S[:, t]\n",
        "            S[:, t+1] = S[:, t] + mu_S * dt + sigma_S * dW1 * sqrt_dt\n",
        "            #S[:, t+1] = S[:, t]  + sigma_S * dW1 * sqrt_dt\n",
        "\n",
        "        # Call option payoff\n",
        "        payoffs = np.maximum(S[:, -1] - K, 0)\n",
        "        discounted = np.exp(-r * T) * payoffs\n",
        "\n",
        "        mean_price = np.mean(discounted)\n",
        "        prices[i] = mean_price\n",
        "\n",
        "    return prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7ikHf_OPIKF"
      },
      "source": [
        "# ***TRAINING***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aruiln1AJTmC"
      },
      "outputs": [],
      "source": [
        "# SABR parameters\n",
        "beta = 1\n",
        "nu = 0.\n",
        "rho = -0.2\n",
        "strike_price = 500\n",
        "T = 1.0\n",
        "N = 365\n",
        "F0 = 1200\n",
        "sigma0 = 0.25\n",
        "r=0.05\n",
        "\n",
        "F, sigma = simulate_sabr_path(beta, nu, rho, F0, sigma0, strike_price, T, N, r, plot=False)\n",
        "\n",
        "F_torch = torch.tensor(F, dtype=torch.float32, requires_grad=True).view(-1, 1)\n",
        "sigma_torch = torch.tensor(sigma, dtype=torch.float32, requires_grad=True).view(-1, 1)\n",
        "\n",
        "F_min, F_max = torch.min(F_torch).item(), torch.max(F_torch).item()\n",
        "v_min, v_max = torch.min(sigma_torch).item(), torch.max(sigma_torch).item()\n",
        "\n",
        "kappa = 0\n",
        "theta = 0\n",
        "sigma0 = 0.\n",
        "rho = -0.2\n",
        "r = 0.05\n",
        "\n",
        "lambda1, lambda2 = 1, 1\n",
        "\n",
        "model = Alternative_FCN()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=2e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=200,\n",
        "    min_lr=2e-5)\n",
        "\n",
        "best_loss = float('inf')\n",
        "save_path = 'best_model.pth'\n",
        "\n",
        "totallosses=[]\n",
        "Lic_list = []\n",
        "Lpde_list = []\n",
        "Lbd_list = []\n",
        "Lbd_Smax_list = []\n",
        "Lbd_S0_list = []\n",
        "Lbd_Vmax_list = []\n",
        "Lbd_V0_list = []\n",
        "\n",
        "# Compute model\n",
        "\n",
        "for i in range(1000):\n",
        "    data = generate_points_per_condition(N=1000, S_min=0.0, S_max=1.2*F_max,\n",
        "                                    sigma_min=0.0, sigma_max=1.1*v_max, t=1.0)\n",
        "\n",
        "    t0 = data['initial'].clone().detach().requires_grad_(True)\n",
        "    V0 = data['bd_sigma0'].clone().detach().requires_grad_(True)\n",
        "    S0 = data['bd_S0'].clone().detach().requires_grad_(True)\n",
        "    Smax = data['bd_Smax'].clone().detach().requires_grad_(True)\n",
        "    vmax = data['bd_sigmamax'].clone().detach().requires_grad_(True)\n",
        "    inputs = data['interior'].clone().detach().requires_grad_(True)\n",
        "    inputs_all = data['all'].clone().detach().requires_grad_(True)\n",
        "\n",
        "\n",
        "    C0 = model(t0)\n",
        "    loss1 = (torch.squeeze(C0) - IC_european_call(t0[:,0].requires_grad_(True).reshape(-1, 1), strike_price))**2\n",
        "    Lic = lambda1 * loss1.mean()\n",
        "\n",
        "    # Compute the PDE residual as the loss\n",
        "    residual = heston_pde(model, inputs[:,0].requires_grad_(True).reshape(-1, 1), inputs[:,1].requires_grad_(True).reshape(-1, 1), inputs[:,2].requires_grad_(True).reshape(-1, 1),  kappa, theta, sigma, rho, r)\n",
        "    Lpde = lambda2 * torch.mean(residual**2)\n",
        "\n",
        "    bnd_S_min = Smin_conditions(model, S0)\n",
        "\n",
        "    bnd_S_max = Smax_conditions(model, Smax)\n",
        "\n",
        "    bnd_v_min = v0_conditions(model, V0, strike_price, r)\n",
        "\n",
        "    bnd_v_max = Vmax_conditions(model, vmax)\n",
        "\n",
        "    Lbd = ( 10000*bnd_S_max + 10*bnd_v_max + 10*bnd_v_min + 1*bnd_S_min )\n",
        "    Lic2 = 0.00000015*Lic\n",
        "    Lpde2 = Lpde\n",
        "    loss = Lic2 + Lpde2 + Lbd\n",
        "\n",
        "    loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    scheduler.step(loss)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    totallosses.append(loss.item())\n",
        "    Lic_list.append(Lic2.item())\n",
        "    Lpde_list.append(Lpde2.item())\n",
        "    Lbd_list.append(Lbd.item())\n",
        "    Lbd_Smax_list.append(10000*bnd_S_max.item())\n",
        "    Lbd_S0_list.append(1*bnd_S_min.item())\n",
        "    Lbd_Vmax_list.append(10*bnd_v_max.item())\n",
        "    Lbd_V0_list.append(10*bnd_v_min.item())\n",
        "\n",
        "    if i % 100 == 0:\n",
        "            print(f\"Iteration {i}, Loss: {loss.item()}\")\n",
        "            print(f\"Loss-1 (IC): {Lic2.item()}, Loss-2 (pde): {Lpde2.item()} , Loss-5 (bd): {Lbd.item()}\")\n",
        "            print(\"CURRENT LR:\", scheduler.get_last_lr())\n",
        "\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': best_loss,\n",
        "            'iteration': i\n",
        "            }, save_path)\n",
        "\n",
        "print(\"best loss: \", best_loss )\n",
        "\n",
        "checkpoint = torch.load(\"best_model.pth\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "print(f\"Model {checkpoint['iteration']} iterasyondan yüklendi, Loss: {checkpoint['loss']:.6f}\")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(len(totallosses)), totallosses, label='Total Loss')\n",
        "plt.plot(range(len(totallosses)), Lic_list, label='Inital Con. Loss')\n",
        "plt.plot(range(len(totallosses)), Lpde_list, label='Physics Loss')\n",
        "plt.plot(range(len(totallosses)), Lbd_list, label='Boundary Con. Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epoch')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IEfnQG8NWyk"
      },
      "source": [
        "#  ***Result on Syntetic Dataset by using SABR Path***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_df = pd.DataFrame(columns=[\"Total\", \"ic\", \"bd\", \"phy\",\"Smax\", \"S0\", \"Vmax\", \"V0\"])\n",
        "\n",
        "# Output directory to save the plots\n",
        "output_dir = \"/content/drive/MyDrive/Bitirme_real_dataset/new/Heston\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "loss_df = pd.DataFrame({\n",
        "    \"Total\": totallosses,\n",
        "    \"ic\": Lic_list,\n",
        "    \"bd\": Lbd_list,\n",
        "    \"phy\": Lpde_list,\n",
        "    \"Smax\": Lbd_Smax_list,\n",
        "    \"S0\": Lbd_S0_list,\n",
        "    \"Vmax\": Lbd_Vmax_list,\n",
        "    \"V0\": Lbd_V0_list\n",
        "})\n",
        "\n",
        "excel_path = os.path.join(output_dir, \"loss.xlsx\")\n",
        "\n",
        "loss_df.to_excel(excel_path, index=False)\n",
        "\n",
        "print(f\"Results saved to : {excel_path}\")"
      ],
      "metadata": {
        "id": "7obygXPV8XYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all sheets in the Excel file\n",
        "excel_file = \"/content/drive/MyDrive/Bitirme_real_dataset/option_price_comparison_beta1_nu0.xlsx\"\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "prediction_df = pd.DataFrame(columns=[\"Prediction\", \"MC\", \"BS\", \"S\", \"Sigma\", \"t\"])\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/Bitirme_real_dataset/new/Heston\"\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "koAsy6b09Arj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v-hMXN3JQQr"
      },
      "outputs": [],
      "source": [
        "strike_price = 50\n",
        "r=0.05\n",
        "\n",
        "F = df['S'].values  # SPOT PRICE\n",
        "sigma = df['Sigma'].values\n",
        "mc_prices = df['MC'].values\n",
        "bs_prices = df['BS'].values\n",
        "time = df['t'].values\n",
        "\n",
        "F_torch = torch.tensor(F, dtype=torch.float32, requires_grad=True).view(-1, 1)\n",
        "sigma_torch = torch.tensor(sigma, dtype=torch.float32, requires_grad=True).view(-1, 1)\n",
        "time_torch = torch.tensor(time, dtype=torch.float32, requires_grad=True).view(-1, 1)\n",
        "\n",
        "inputs_heston = torch.cat([F_torch, sigma_torch, time_torch], dim=1)\n",
        "\n",
        "C = model(inputs_heston) # PINN OPTION PRICES\n",
        "C = C.detach().numpy()\n",
        "\n",
        "plt.plot(time, C, label=\"PINN\", linestyle='dashed')\n",
        "plt.plot(time, bs_prices, label=\"Analytical BS Values\")\n",
        "plt.plot(time, mc_prices, label=\"MC\")\n",
        "plt.xlabel(\"Time (t)\")\n",
        "plt.ylabel(\"Option Price (C)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(F, C, label=\"PINN\", marker='o')\n",
        "plt.scatter(F, bs_prices, label=\"Analytical BS Values\", marker=\"x\")\n",
        "plt.scatter(F, mc_prices, label=\"MC\", marker=\"x\")\n",
        "plt.xlabel(\"Stock Price (S)\")\n",
        "plt.ylabel(\"Option Price (C)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_df = pd.DataFrame({\n",
        "    \"Prediction\": C.flatten(),\n",
        "    \"MC\": mc_prices,\n",
        "    \"BS\": bs_prices,\n",
        "    \"S\": F,\n",
        "    \"Sigma\": sigma,\n",
        "    \"t\": time\n",
        "})\n",
        "\n",
        "excel_path = os.path.join(output_dir, \"option_price_comparison_syn.xlsx\")\n",
        "\n",
        "prediction_df.to_excel(excel_path, index=False)\n",
        "\n",
        "print(f\"Sonuçlar başarıyla şuraya kaydedildi: {excel_path}\")"
      ],
      "metadata": {
        "id": "qEL2XuyW9ZC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5iLZeweNw_A"
      },
      "source": [
        "# ***RESULTS ON RANDOM DATASET***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zg_1GitNwvY"
      },
      "outputs": [],
      "source": [
        "# Load all sheets in the Excel file\n",
        "excel_file2 = \"/content/drive/MyDrive/Bitirme_real_dataset/option_price_comparison_random.xlsx\"\n",
        "df2 = pd.read_excel(excel_file2)\n",
        "\n",
        "F = df2['S'].values  # SPOT PRICE\n",
        "sigma = df2['Sigma'].values\n",
        "mc_prices = df2['MC'].values\n",
        "bs_prices = df2['BS'].values\n",
        "time = df2['t'].values\n",
        "\n",
        "F_torch = torch.tensor(F, dtype=torch.float32, requires_grad=True).view(-1, 1)\n",
        "sigma_torch = torch.tensor(sigma, dtype=torch.float32, requires_grad=True).view(-1, 1)\n",
        "time_torch = torch.tensor(time, dtype=torch.float32, requires_grad=True).view(-1, 1)\n",
        "\n",
        "inputs_heston_random = torch.cat([F_torch, sigma_torch, time_torch], dim=1)\n",
        "\n",
        "predictionrandom_df = pd.DataFrame(columns=[\"Prediction\", \"MC\", \"BS\", \"S\", \"Sigma\", \"t\"])\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/Bitirme_real_dataset/new/Heston\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "C = model(inputs_heston_random)\n",
        "C = C.detach().numpy()\n",
        "\n",
        "C = C.reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7ryfn2IQDSh"
      },
      "outputs": [],
      "source": [
        "predictionrandom_df = pd.DataFrame({\n",
        "    \"Prediction\": C.flatten(),\n",
        "    \"MC\": mc_prices,\n",
        "    \"BS\": bs_prices,\n",
        "    \"S\": F,\n",
        "    \"Sigma\": sigma,\n",
        "    \"t\": time\n",
        "})\n",
        "\n",
        "excel_path = os.path.join(output_dir, \"option_price_comparison_rnd.xlsx\")\n",
        "\n",
        "predictionrandom_df.to_excel(excel_path, index=False)\n",
        "\n",
        "print(f\"Results saved to : {excel_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHl0F0dhjoU6"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}